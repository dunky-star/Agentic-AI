version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      # Pick your small q4 model here
      MODEL: "gemma2:2b-instruct-q4_0"

      # Keep memory usage gentle for an 8 GB MacBook
      OLLAMA_NUM_PARALLEL: "1"
      OLLAMA_KV_CACHE_TYPE: "q4_0"

      # Reasonable defaults you can override per-request
      OLLAMA_NUM_CTX: "2048"
      OLLAMA_NUM_PREDICT: "256"

      # Allow local browser UIs if you add one later
      OLLAMA_ORIGINS: "*"

    volumes:
      - ollama_data:/root/.ollama

    # Keep Dockerâ€™s memory footprint small so macOS stays snappy
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2"

    # Healthcheck that uses the ollama CLI (no curl/bash needed)
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 10s

  # Pulls your chosen model after the API is reachable
  model-puller:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/sh", "-lc"]
    command: >
      'until ollama list >/dev/null 2>&1; do sleep 2; done;
       echo "Pulling ${MODEL:-gemma2:2b-instruct-q4_0} ...";
       ollama pull ${MODEL:-gemma2:2b-instruct-q4_0};
       echo "Model pull complete."'

volumes:
  ollama_data:
